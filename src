######################################################################################
# nltk - "natural language toolkit" is a python library with support for 
#         natural language processing. Super-handy.
# Specifically, we will use 2 functions from nltk
#  sent_tokenize: given a group of text, tokenize (split) it into sentences
#  word_tokenize: given a group of text, tokenize (split) it into words
#  stopwords.words('english') to find and ignored very common words ('I', 'the',...) 
#  to use stopwords, you need to have run nltk.download() first - one-off setup
######################################################################################

from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords

######################################################################################
# defaultdict is a class that inherits from dictionary, but has
# one additional nice feature: Usually, a Python dictionary throws a KeyError if you try 
# to get an item with a key that is not currently in the dictionary. 
# The defaultdict in contrast will simply create any items that you try to access 
# (provided of course they do not exist yet). To create such a "default" item, it relies 
# a function that is passed in..more below. 
######################################################################################
from collections import defaultdict

######################################################################################
#  punctuation to ignore punctuation symbols
######################################################################################
from string import punctuation

######################################################################################
# heapq.nlargest is a function that given a list, easily and quickly returns
# the 'n' largest elements in the list. More below
######################################################################################
from heapq import nlargest

######################################################################################
# Our first class, named FrequencySummarizer 
######################################################################################

class FrequencySummarizer:
    def __init__(self, min_cut=0.1, max_cut=0.9):
    #Constructor - function that's called each time an object of this class is instantiated.
        self._min_cut = min_cut
        self._max_cut = max_cut
        self._stopwords = set(stopwords.words('english') + list(punctuation))
        
    def _compute_frequencies(self, word_sent):
        #This function will take in a list of sentences and return a dictionary with keys as 
        #each unique word in the sentence and value as the frequency of the set for that word.
        freq = defaultdict(int)
        for sentence in word_sent:
            for word in sentence:
                if word not in self._stopwords:
                    freq[word] += 1
        max_freq=float(max(freq.values()))
        for word in list(freq.keys()):
            freq[word] = freq[word]/max_freq
            if freq[word] >= self._max_cut or freq[word] <= self._min_cut:
                del freq[word]
        return freq
    
    def summarize(self, text, n):
        sents = sent_tokenize(text)
        assert n <= len(sents)#if the summary length is more than the article, the program will throw an error.
        word_sent = [word_tokenize(s.lower()) for s in sents]
        self._freq = self._compute_frequencies(word_sent)
        ranking = defaultdict(int)
        
        for i, sent in enumerate(word_sent):
            #Let's say you have a list ['a', 'b', 'c']. The result of using enumerate(list) is [(0, a), (1, b), (2, c)]
            #This elimnates the need to have a counter variable.
            for word in sent:
                if word in self._freq:
                    ranking[i] += self._freq[word]
                    #["Mary had a little lamb", "The quick brown fox"]. We use the enumerate function so we get 
                    #[(0, "Mary had a little lamb"), (1, "The quick brown fox")]. Now, we calculate sum of 
                    #frequencies of individual words ranking is freq('Mary') + freq('little') + freq('lamb')
        sents_idx = nlargest(n, ranking, key = ranking.get)
        return [sents[j] for j in sents_idx]

############################################################################################################        
#Now to get a URL and summarize
#############################################################################################################
import urllib
from bs4 import BeautifulSoup

def get_only_text_washingtonpost_url(url):
    #Takes in URL and returns only article text. Specific to washington post.
    page = urllib.urlopen(url).read().decode('utf8')
    soup = BeautifulSoup(page)
    text = ' '.join(map(lambda p: p.text, soup.find_all('body')))
    soup2 = BeautifulSoup(text)
    text = ' '.join(map(lambda p: p.text, soup2.find_all('p'))) 
    return soup.title.text, text
    
#####################################################################################
# OK! Now lets give this code a spin
#####################################################################################
someUrl = "https://www.washingtonpost.com/news/the-switch/wp/2015/08/06/why-kids-are-meeting-more-strangers-online-than-ever-before/"
# the article we would like to summarize
textOfUrl = get_only_text_washington_post_url(someUrl)
# get the title and text
fs = FrequencySummarizer()
# instantiate our FrequencySummarizer class and get an object of this class
summary = fs.summarize(textOfUrl[1], 3)
# get a summary of this article that is 3 sentences long

